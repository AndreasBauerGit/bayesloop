#!/usr/bin/env python
"""
This file introduces an extension to the basic Study-class that allows to compute the distribution of hyper-parameters.
"""

from .study import *
from .preprocessing import *
from scipy.misc import logsumexp
from scipy.misc import factorial
from mpl_toolkits.mplot3d import Axes3D
from copy import copy
import sympy.abc as abc
from sympy import lambdify
import sympy.stats
from sympy.stats import density


class HyperStudy(Study):
    """
    This class serves as an extension to the basic Study class and allows to compute the distribution of hyper-
    parameters of a given transition model. For further information, see the documentation of the fit-method of this
    class.
    """
    def __init__(self):
        super(HyperStudy, self).__init__()

        self.hyperGrid = []
        self.hyperGridValues = []
        self.hyperGridConstant = []
        self.hyperPrior = None
        self.hyperPriorValues = []
        self.hyperParameterDistribution = None
        self.averagePosteriorSequence = None
        self.logEvidenceList = []
        self.localEvidenceList = []

        print '  --> Hyper-study'

    def setHyperGrid(self, hyperGrid, silent=False):
        """
        Creates a regular grid of hyper-parameter values to be analyzed by the fit-method of the HyperStudy class.

        Parameters:
            hyperGrid - List of lists with each containing the name of a hyper-parameter together with a lower and upper
                boundary as well as a number of steps in between.
                Example: hyperGrid = [['sigma', 0, 1, 20],['log10pMin', -10, -5, 10]]

            silent - If set to True, no output is generated by this method.

        Returns:
            None
        """
        # in case no hyper-grid is provided, return directly
        if not hyperGrid:
            print '! No hyper-grid provided.'
            return

        self.hyperGrid = hyperGrid

        # create array with hyper-grid values
        temp = np.meshgrid(*[np.linspace(lower, upper, steps) for name, lower, upper, steps in self.hyperGrid],
                           indexing='ij')
        self.hyperGridValues = np.array([t.ravel() for t in temp]).T
        self.hyperGridConstant = [np.abs(upper-lower)/(float(steps)-1) for name, lower, upper, steps in self.hyperGrid]

        if not silent:
            print '+ Set hyper-grid for the following hyper-parameters:'
            print '  {}'.format([n for n, l, u, s in self.hyperGrid])

    def setHyperPrior(self, hyperPrior):
        """
        Assigns prior probability values to all points on the hyper-parameter grid that is used in a hyper-study or
        change-point study.

        Parameters:
            hyperPrior - List of SymPy random variables, each of which represents the prior distribution of one hyper-
                parameter. The multiplicative probability (density) will be assigned to the individual raster points.
                Alternatively, a function can be provided that takes exactly as many arguments as there are hyper-
                parameters in the transition model. The resulting prior distribution is renormalized such that the sum
                over all points specified by the hyper-grid equals one.
        """
        # Actual hyper-parameter prior values have to be evaluated inside the fit-method, because hyper-grid values have
        # to be evaluated first. In the case of a custom hyper-grid, it cannot be ensured, that calling setHyperPrior is
        # possible before the fit-method is called.
        self.hyperPrior = hyperPrior
        print '+ Will use custom hyper-parameter prior.'

    def fit(self, forwardOnly=False, evidenceOnly=False, silent=False, nJobs=1):
        """
        This method over-rides the according method of the Study-class. It runs the algorithm for equally spaced hyper-
        parameter values as defined by the variable 'hyperGrid'. The posterior sequence represents the average
        model of all analyses. Posterior mean values are computed from this average model.

        Parameters:
            forwardOnly - If set to True, the fitting process is terminated after the forward pass. The resulting
                posterior distributions are so-called "filtering distributions" which - at each time step -
                only incorporate the information of past data points. This option thus emulates an online
                analysis.

            evidenceOnly - If set to True, only forward pass is run and evidence is calculated. In contrast to the
                forwardOnly option, no posterior mean values are computed and no posterior distributions are stored.

            silent - If set to true, reduced output is created by this method.

            nJobs - Number of processes to employ. Multiprocessing is based on the 'pathos' module.

        Returns:
            None
        """
        print '+ Started new fit.'

        self.formattedData = movingWindow(self.rawData, self.observationModel.segmentLength)

        if not self.checkConsistency():
            return

        if not self.hyperGrid:
            print '! No hyper-grid defined for hyper-parameter values. Using standard fit-method.'
            Study.fit(self, forwardOnly=forwardOnly, evidenceOnly=evidenceOnly, silent=silent)
            return

        # in case a custom hyper-grid is defined by the user, check if all attributes are set
        if self.hyperGridValues == [] or self.hyperGridConstant == []:
            print "! To set a custom hyper-grid, the attributes 'hyperGrid', 'hyperGridValues' and 'hyperGridConstant'"\
                  "  have to be set manually. Using standard fit-method now."

        # determine prior distribution
        # check whether function is provided
        if hasattr(self.hyperPrior, '__call__'):
            try:
                self.hyperPriorValues = [self.hyperPrior(*value) for value in self.hyperGridValues]
                self.hyperPriorValues /= np.sum(self.hyperPriorValues)  # renormalize hyper-parameter prior
                if not silent:
                    print '+ Set custom hyper-parameter prior: {}'.format(self.hyperPrior.__name__)
            except:
                print '! Failed to set hyper-parameter prior. Check number of variables of passed function.'
                self.hyperPriorValues = None

        # check whether single random variable is provided
        elif type(self.hyperPrior) is sympy.stats.rv.RandomSymbol:
            self.hyperPrior = [self.hyperPrior]

        # check if list/tuple is provided
        elif isinstance(self.hyperPrior, (list, tuple)) and not isinstance(self.hyperPrior, basestring):
            # check if given prior is correctly formatted to fit length of hyper-grid array.
            # we use 'len(self.hyperGridValues[0])' because self.hyperGrid is reformatted within changepointStudy, when
            # using break-points.
            if len(self.hyperPrior) != len(self.hyperGridValues[0]):
                print '! {} hyper-parameters are specified in hyper-grid. Priors are provided for {}.'\
                    .format(len(self.hyperGridValues[0]), len(self.hyperPrior))
                self.hyperPriorValues = None
                return
            else:
                if not silent:
                    print '+ Setting custom hyper-parameter priors'
                self.hyperPriorValues = np.ones(len(self.hyperGridValues))
                for i, rv in enumerate(self.hyperPrior):  # loop over all specified priors
                    if len(list(rv._sorted_args[0].distribution.free_symbols)) > 0:
                        print '! Prior distribution must not contain free parameters.'
                        self.hyperPriorValues = None
                        return

                    # get symbolic representation of probability density
                    x = abc.x
                    symDensity = density(rv)(x)
                    print '  {}'.format(symDensity)

                    # get density as lambda function
                    pdf = lambdify([x], symDensity, modules=['numpy', {'factorial': factorial}])

                    # update hyper-parameter prior
                    self.hyperPriorValues *= pdf(self.hyperGridValues[:, i])

                # renormalize hyper-parameter prior
                self.hyperPriorValues /= np.sum(self.hyperPriorValues)

        # if no hyper-prior could be assigned, or the one defined does not fit, assign flat prior
        if len(self.hyperPriorValues) != len(self.hyperGridValues):
            self.hyperPriorValues = np.ones(len(self.hyperGridValues))/len(self.hyperGridValues)

        if not evidenceOnly:
            self.averagePosteriorSequence = np.zeros([len(self.formattedData)]+self.gridSize)

        self.logEvidenceList = []
        self.localEvidenceList = []

        # we use the setSelectedHyperParameters-method from the Study class
        self.selectedHyperParameters = [name for name, lower, upper, steps in self.hyperGrid]

        print '    + {} analyses to run.'.format(len(self.hyperGridValues))

        # check if multiprocessing is available
        if nJobs > 1:
            try:
                from pathos.multiprocessing import ProcessPool
            except:
                print "! Install 'pathos.multiprocessing' to enable multiprocessing."
                print "! Switching back to single process."
                nJobs=1

        # prepare parallel execution if necessary
        if nJobs > 1:
            print '    + Creating {} processes.'.format(nJobs)
            pool = ProcessPool(nodes=nJobs)

            # use parallelFit method to create copies of this HyperStudy instance with only partial hyper-grid values
            subStudies = pool.map(self.parallelFit,
                                  range(nJobs),
                                  [nJobs]*nJobs,
                                  [forwardOnly]*nJobs,
                                  [evidenceOnly]*nJobs,
                                  [silent]*nJobs)

            # merge all sub-studies
            for S in subStudies:
                self.logEvidenceList += S.logEvidenceList
                self.localEvidenceList += S.localEvidenceList
                if not evidenceOnly:
                    self.averagePosteriorSequence += S.averagePosteriorSequence
        # single process fit
        else:
            for i, hyperParamValues in enumerate(self.hyperGridValues):
                self.setSelectedHyperParameters(hyperParamValues)

                # call fit method from parent class
                Study.fit(self, forwardOnly=forwardOnly, evidenceOnly=evidenceOnly, silent=True)

                self.logEvidenceList.append(self.logEvidence)
                self.localEvidenceList.append(self.localEvidence)
                if not evidenceOnly:
                    self.averagePosteriorSequence += self.posteriorSequence *\
                                                     np.exp(self.logEvidence - self.logEvidenceList[0]) *\
                                                     self.hyperPriorValues[i]

                if not silent:
                    print '    + Analysis #{} of {} -- Hyper-parameter values {} -- log10-evidence = {:.5f}'\
                        .format(i+1, len(self.hyperGridValues), hyperParamValues, self.logEvidence / np.log(10))

        # reset list of parameters to optimize, so that unpacking and setting hyper-parameters works as expected
        self.selectedHyperParameters = []

        if not evidenceOnly:
            # compute average posterior distribution
            normalization = np.array([np.sum(posterior) for posterior in self.averagePosteriorSequence])
            for i in range(len(self.grid)):
                normalization = normalization[:, None]  # add axis; needs to match averagePosteriorSequence
            self.averagePosteriorSequence /= normalization

            # set self.posteriorSequence to average posterior sequence for plotting reasons
            self.posteriorSequence = self.averagePosteriorSequence

            if not silent:
                print '    + Computed average posterior sequence'

        # compute log-evidence of average model
        self.logEvidence = logsumexp(np.array(self.logEvidenceList) + np.log(self.hyperPriorValues))

        print '    + Log10-evidence of average model: {:.5f}'.format(self.logEvidence / np.log(10))

        # compute hyper-parameter distribution
        logHyperParameterDistribution = self.logEvidenceList + np.log(self.hyperPriorValues)
        scaledLogHyperParameterDistribution = logHyperParameterDistribution - np.mean(logHyperParameterDistribution)
        self.hyperParameterDistribution = np.exp(scaledLogHyperParameterDistribution)
        self.hyperParameterDistribution /= np.sum(self.hyperParameterDistribution)
        self.hyperParameterDistribution /= np.prod(self.hyperGridConstant)  # probability density

        if not silent:
            print '    + Computed hyper-parameter distribution'

        # compute local evidence of average model
        self.localEvidence = np.sum((np.array(self.localEvidenceList).T*self.hyperParameterDistribution).T, axis=0)

        if not silent:
            print '    + Computed local evidence of average model'

        # compute posterior mean values
        if not evidenceOnly:
            self.posteriorMeanValues = np.empty([len(self.grid), len(self.posteriorSequence)])
            for i in range(len(self.grid)):
                self.posteriorMeanValues[i] = np.array([np.sum(p*self.grid[i]) for p in self.posteriorSequence])

            if not silent:
                print '    + Computed mean parameter values.'

        # clear self.hyperParameterPrior (in case fit is called after changing self.hyperGrid)
        self.hyperPriorValues = None

        # discard evidence values of individual fits
        self.logEvidenceList = []
        self.localEvidenceList = []

        print '+ Finished fit.'

    def parallelFit(self, idx, nJobs, forwardOnly, evidenceOnly, silent):
        """
        This method is called by the fit method of the HyperStudy class. It creates a copy of the current class
        instance and performs a fit based on a subset of the specified hyper-parameter grid. The method thus allows
        to distribute a HyperStudy fit among multiple processes for multiprocessing.

        Parameters:
            idx - Index from 0 to (nJobs-1), indicating which part of the hyper-grid values are to be analyzed.

            nJobs - Number of processes to employ. Multiprocessing is based on the 'pathos' module.

            forwardOnly - If set to True, the fitting process is terminated after the forward pass. The resulting
                posterior distributions are so-called "filtering distributions" which - at each time step -
                only incorporate the information of past data points. This option thus emulates an online
                analysis.

            evidenceOnly - If set to True, only forward pass is run and evidence is calculated. In contrast to the
                forwardOnly option, no posterior mean values are computed and no posterior distributions are stored.

            customHyperGrid - If set to True, the keyword argument 'hyperGrid' will not be used. Instead, all relevant
                attributes have to be set manually by the user. May be used for irregular grids of hyper-parameter
                values.

            silent - If set to True, no output is generated by the fitting method.

        Returns:
            HyperStudy instance
        """
        S = copy(self)
        S.hyperGridValues = np.array_split(S.hyperGridValues, nJobs)[idx]
        S.hyperPriorValues = np.array_split(S.hyperPriorValues, nJobs)[idx]

        for i, hyperParamValues in enumerate(S.hyperGridValues):
            S.setSelectedHyperParameters(hyperParamValues)

            # call fit method from parent class
            Study.fit(S, forwardOnly=forwardOnly, evidenceOnly=evidenceOnly, silent=True)

            S.logEvidenceList.append(S.logEvidence)
            S.localEvidenceList.append(S.localEvidence)
            if not evidenceOnly:
                S.averagePosteriorSequence += S.posteriorSequence *\
                                              np.exp(S.logEvidence) *\
                                              S.hyperPriorValues[i]

            if not silent:
                print '    + Process {} -- Analysis #{} of {}'.format(idx, i+1, len(S.hyperGridValues))

        print '    + Process {} finished.'.format(idx)
        return S

    # optimization methods are inherited from Study class, but cannot be used in this case
    def optimize(self, *args, **kwargs):
        print "! 'HyperStudy' object has no attribute 'optimize'"
        return

    def optimizationStep(self, *args, **kwargs):
        print "! 'HyperStudy' object has no attribute 'optimizationStep'"
        return

    def plotHyperParameterDistribution(self, param=0, **kwargs):
        """
        Creates a bar chart of a hyper-parameter distribution done with the HyperStudy class. The distribution is
        marginalized with respect to the hyper-parameter passed by name or index.

        Parameters:
            param - Parameter name or index of hyper-parameter to display; default: 0 (first model hyper-parameter)
            **kwargs - All further keyword-arguments are passed to the bar-plot (see matplotlib documentation)

        Returns:
            Two numpy arrays. The first array contains the hyper-parameter values, the second one the
            corresponding probability (density) values
        """
        hyperParameterNames = [name for name, lower, upper, steps in self.hyperGrid]

        if isinstance(param, (int, long)):
            paramIndex = param
        elif isinstance(param, basestring):
            paramIndex = -1
            for i, name in enumerate(hyperParameterNames):
                if name == param:
                    paramIndex = i

            # check if match was found
            if paramIndex == -1:
                print '! Wrong hyper-parameter name. Available options: {0}'.format(hyperParameterNames)
                return
        else:
            print '! Wrong parameter format. Specify parameter via name or index.'
            return

        axesToMarginalize = range(len(hyperParameterNames))
        axesToMarginalize.remove(paramIndex)

        # reshape hyper-parameter distribution for easy marginalizing
        hyperGridSteps = [steps for name, lower, upper, steps in self.hyperGrid]
        distribution = self.hyperParameterDistribution.reshape(hyperGridSteps, order='C')
        marginalDistribution = np.squeeze(np.apply_over_axes(np.sum, distribution, axesToMarginalize))

        # marginal distribution is not created by sum, but by the integral
        integrationFactor = np.prod([self.hyperGridConstant[axis] for axis in axesToMarginalize])
        marginalDistribution *= integrationFactor

        x = np.linspace(*self.hyperGrid[paramIndex][1:])
        plt.bar(x, marginalDistribution, align='center', width=self.hyperGridConstant[paramIndex], **kwargs)

        plt.xlabel(hyperParameterNames[paramIndex])

        # in case an integer step size for hyper-parameter values is chosen, probability is displayed
        # (probability density otherwise)
        if self.hyperGridConstant[paramIndex] == 1.:
            plt.ylabel('probability')
        else:
            plt.ylabel('probability density')

        return x, marginalDistribution

    def plotJointHyperParameterDistribution(self, params=[0, 1], figure=None, subplot=111,
                                            **kwargs):
        """
        Creates a 3D bar chart of a joint hyper-parameter distribution (of two hyper-parameters) done with the
        HyperStudy class. The distribution is marginalized with respect to the hyper-parameters passed by names or
        indices. Note that the 3D plot can only be included in an existing plot by passing a figure object and subplot
        specification.

        Parameters:
            params - List of two parameter names or indices of hyper-parameters to display; default: [0, 1]
                (first and second model parameter)

            figure - In case the plot is supposed to be part of an existing figure, it can be passed to the method. By
                default, a new figure is created.

            subplot - Characterization of subplot alignment, as in matplotlib. Default: 111

            **kwargs - all further keyword-arguments are passed to the bar3d-plot (see matplotlib documentation)

        Returns:
            Three numpy arrays. The first and second array contains the hyper-parameter values, the
            third one the corresponding probability (density) values
        """
        hyperParameterNames = [name for name, lower, upper, steps in self.hyperGrid]

        # check if list with two elements is provided
        if not isinstance(params, (list, tuple)):
            print '! A list of exactly two hyper-parameters has to be provided.'
            return
        elif not len(params) == 2:
            print '! A list of exactly two hyper-parameters has to be provided.'
            return

        # check for type of parameters (indices or names)
        if all(isinstance(p, (int, long)) for p in params):
            paramIndices = params
        elif all(isinstance(p, basestring) for p in params):
            paramIndices = []
            for i, name in enumerate(hyperParameterNames):
                for p in params:
                    if name == p:
                        paramIndices.append(i)

            # check if match was found
            if paramIndices == []:
                print '! Wrong hyper-parameter name. Available options: {0}'.format(hyperParameterNames)
                return
        else:
            print '! Wrong parameter format. Specify parameters either via name or index.'
            return

        # check if one of the parameter names provided is wrong
        if not len(paramIndices) == 2:
            print '! Probably one wrong hyper-parameter name. Available options: {0}'.format(hyperParameterNames)

        # check if parameter indices are in ascending order (so axes are labeled correctly)
        if not paramIndices[0] < paramIndices[1]:
            print '! Switching hyper-parameter order for plotting.'
            paramIndices = paramIndices[::-1]

        axesToMarginalize = range(len(hyperParameterNames))
        for p in paramIndices:
            axesToMarginalize.remove(p)

        # reshape hyper-parameter distribution for easy marginalizing
        hyperGridSteps = [steps for name, lower, upper, steps in self.hyperGrid]
        distribution = self.hyperParameterDistribution.reshape(hyperGridSteps, order='C')
        marginalDistribution = np.squeeze(np.apply_over_axes(np.sum, distribution, axesToMarginalize))

        # marginal distribution is not created by sum, but by the integral
        integrationFactor = np.prod([self.hyperGridConstant[axis] for axis in axesToMarginalize])
        marginalDistribution *= integrationFactor

        x, y = np.meshgrid(np.linspace(*self.hyperGrid[paramIndices[0]][1:]),
                           np.linspace(*self.hyperGrid[paramIndices[1]][1:]), indexing='ij')
        z = marginalDistribution
        print np.amax(z)

        # allow to add plot to predefined figure
        if figure is None:
            fig = plt.figure()
        else:
            fig = figure
        ax = fig.add_subplot(subplot, projection='3d')

        ax.bar3d(x.flatten() - self.hyperGridConstant[paramIndices[0]]/2.,
                 y.flatten() - self.hyperGridConstant[paramIndices[1]]/2.,
                 z.flatten()*0.,
                 self.hyperGridConstant[paramIndices[0]],
                 self.hyperGridConstant[paramIndices[1]],
                 z.flatten(),
                 zsort='max',
                 **kwargs
                 )

        ax.set_xlabel(hyperParameterNames[paramIndices[0]])
        ax.set_ylabel(hyperParameterNames[paramIndices[1]])

        # in case an integer step size for hyper-parameter values is chosen, probability is displayed
        # (probability density otherwise)
        if self.hyperGridConstant[paramIndices[0]]*self.hyperGridConstant[paramIndices[1]] == 1.:
            ax.set_zlabel('probability')
        else:
            ax.set_zlabel('probability density')

        return x, y, marginalDistribution
